"""
é€šç”¨åŒ–æ¨¡å‹æµ‹è¯•å‡½æ•°,ç”¨äºæŸ¥çœ‹æ¨¡å‹ç»“æ„ã€æ£€æµ‹æ˜¯å¦æ­£å¸¸å‰å‘å’Œåå‘ä¼ æ’­ï¼š
1. æ¥å—ä»»æ„æ¨¡å‹å®ä¾‹åŒ–å¯¹è±¡ `model`ã€‚
2. è‡ªå®šä¹‰è¾“å…¥ `inputs` å’Œæ ‡ç­¾ `labels`ã€‚
3. æ”¯æŒå‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­ã€æŸå¤±è®¡ç®—ã€‚
4. å¯¼å‡º ONNX æ¨¡å‹å¹¶éªŒè¯ã€‚
5. è¾“å‡ºæ¨¡å‹è¯¦ç»†ä¿¡æ¯ã€‚
å‚æ•°ï¼š
- model: PyTorch æ¨¡å‹å®ä¾‹åŒ–å¯¹è±¡: torch.nn.Module
- inputs: æ¨¡å‹çš„è¾“å…¥å¼ é‡: tensor æˆ– tulple(tensor1, tensor2, ...) æˆ– list(tensor1, tensor2, ...)
- labels: æ¨¡å‹çš„çœŸå®æ ‡ç­¾å¼ é‡ï¼ˆç”¨äºæŸå¤±è®¡ç®—ï¼‰: tensor
- criterion: æŸå¤±å‡½æ•°å®ä¾‹åŒ–å¯¹è±¡ï¼Œé»˜è®¤ä¸º nn.MSELoss
- optimizer: ä¼˜åŒ–å™¨å®ä¾‹åŒ–å¯¹è±¡ï¼Œé»˜è®¤ä¸º Adam
- onnx_file_path: å¯¼å‡ºçš„ ONNX æ–‡ä»¶è·¯å¾„
"""
import warnings
warnings.filterwarnings('ignore')
import torch
import torch.nn as nn
import torch.onnx
from torchinfo import summary
from torchviz import make_dot

# @profile
def test_model(
    model,
    inputs,
    labels,
    criterion=None,
    optimizer=None,
    onnx_file_path="model_test.onnx"
):
    """
    é€šç”¨åŒ–æ¨¡å‹æµ‹è¯•å‡½æ•°ï¼š
    1. æ¥å—ä»»æ„æ¨¡å‹å®ä¾‹åŒ–å¯¹è±¡ `model`ã€‚
    2. è‡ªå®šä¹‰è¾“å…¥ `inputs` å’Œæ ‡ç­¾ `labels`ã€‚
    3. æ”¯æŒå‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­ã€æŸå¤±è®¡ç®—ã€‚
    4. å¯¼å‡º ONNX æ¨¡å‹å¹¶éªŒè¯ã€‚
    5. è¾“å‡ºæ¨¡å‹è¯¦ç»†ä¿¡æ¯ã€‚
    
    å‚æ•°ï¼š
    - model: PyTorch æ¨¡å‹å®ä¾‹åŒ–å¯¹è±¡: torch.nn.Module
    - inputs: æ¨¡å‹çš„è¾“å…¥å¼ é‡: tensor æˆ– tulple(tensor1, tensor2, ...) æˆ– list(tensor1, tensor2, ...)
    - labels: æ¨¡å‹çš„çœŸå®æ ‡ç­¾å¼ é‡ï¼ˆç”¨äºæŸå¤±è®¡ç®—ï¼‰: tensor
    - criterion: æŸå¤±å‡½æ•°å®ä¾‹åŒ–å¯¹è±¡ï¼Œé»˜è®¤ä¸º nn.MSELoss
    - optimizer: ä¼˜åŒ–å™¨å®ä¾‹åŒ–å¯¹è±¡ï¼Œé»˜è®¤ä¸º Adam
    - onnx_file_path: å¯¼å‡ºçš„ ONNX æ–‡ä»¶è·¯å¾„
    """
    # é»˜è®¤æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    if criterion is None:
        criterion = nn.MSELoss()
    if optimizer is None:
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    # å°†æ¨¡å‹è®¾ä¸ºè®­ç»ƒæ¨¡å¼
    model.train()
    print("\n~~~~~~~~~~~~~~~~~~~ ğŸš€ğŸš€ å¼€å§‹æµ‹è¯•ç¥ç»ç½‘ç»œæ¨¡å‹æ˜¯å¦å¯ä»¥æ­£å¸¸è®­ç»ƒ ğŸš€ğŸš€ ~~~~~~~~~~~~~~~~~~~~")
    # æ‰“å°æ¨¡å‹ç»“æ„ä¿¡æ¯
    print("\n============== æ¨¡å‹ç»“æ„ä¿¡æ¯ ==============")
    _input_data = tuple(inputs) if isinstance(inputs, (tuple, list)) else inputs
    summary(
        model,
        input_data=_input_data,
        col_names=["input_size", "output_size", "num_params"],
        depth=3,
        device="cuda" if next(model.parameters()).is_cuda else "cpu"
    )
    
    # å‰å‘ä¼ æ’­ä¸lossè®¡ç®—
    print("\n============== å‰å‘ä¼ æ’­ ==============")
    if isinstance(inputs, (tuple, list)):
        outputs = model(*inputs)
        # ä¸€è¡Œæ‰“å°æ¨¡å‹å„ä¸ªè¾“å…¥inputçš„å½¢çŠ¶
        print(f"âœ” æ¨¡å‹å„ä¸ªè¾“å…¥çš„å½¢çŠ¶ï¼š{[input.shape for input in inputs]}")

    else:
        outputs = model(inputs)
        print(f"âœ” è¾“å…¥å½¢çŠ¶ï¼š{inputs.shape}")

    # åˆå§‹åŒ–losså˜é‡
    loss = None
    
    if isinstance(outputs, (tuple, list)):
        print(f"âœ” æ¨¡å‹å„ä¸ªè¾“å‡ºçš„å½¢çŠ¶ï¼š{[output.shape for output in outputs]}")
        for i, output in enumerate(outputs):
            if labels.shape == output.shape:
                loss = criterion(output, labels)
                print(f"âœ” ç¬¬{i+1}ä¸ªæ¨¡å‹è¾“å‡ºå¯¹åº”äº†ä¸€ä¸ªlosså€¼: {loss.item()}")
                break  # æ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ¹é…çš„è¾“å‡ºå°±åœæ­¢
        
        if loss is None:
            print("âœ˜ æ²¡æœ‰æ‰¾åˆ°ä¸æ ‡ç­¾å½¢çŠ¶åŒ¹é…çš„è¾“å‡ºï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªè¾“å‡ºè®¡ç®—æŸå¤±")
            loss = criterion(outputs[0], labels)
    else:
        print(f"âœ” æ¨¡å‹è¾“å‡ºå½¢çŠ¶ï¼š{outputs.shape}")
        if labels.shape == outputs.shape:
            loss = criterion(outputs, labels)
            print(f"âœ” æŸå¤±å€¼ï¼š{loss.item()}")
        else: 
            print("âœ˜ æ¨¡å‹è¾“å‡ºå½¢çŠ¶ä¸æ ‡ç­¾å½¢çŠ¶ä¸åŒ¹é…ï¼Œå°è¯•è®¡ç®—æŸå¤±å€¼")
            # å°è¯•è®¡ç®—æŸå¤±ï¼Œå³ä½¿å½¢çŠ¶ä¸å®Œå…¨åŒ¹é…
            try:
                loss = criterion(outputs, labels)
                print(f"âœ” å¼ºåˆ¶è®¡ç®—çš„æŸå¤±å€¼ï¼š{loss.item()}")
            except Exception as e:
                print(f"âœ˜ æ— æ³•è®¡ç®—æŸå¤±å€¼: {e}")
                return  # å¦‚æœæ— æ³•è®¡ç®—æŸå¤±ï¼Œæå‰è¿”å›

    # ç¡®ä¿lossä¸ä¸ºNone
    if loss is None:
        print("âœ˜ æ— æ³•è·å¾—æœ‰æ•ˆçš„losså€¼ï¼Œåœæ­¢æµ‹è¯•")
        return

    # åå‘ä¼ æ’­
    print("\n============== åå‘ä¼ æ’­ ==============")
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    print("âœ” åå‘ä¼ æ’­æ­£å¸¸~")

    # å¯è§†åŒ–è®¡ç®—å›¾
    print("\n============== è®¡ç®—å›¾å¯è§†åŒ– ==============")
    graph = make_dot(loss, params=dict(model.named_parameters()))
    graph.render("model_computation_graph", format="png")
    print("âœ” è®¡ç®—å›¾å·²ä¿å­˜ä¸º 'model_computation_graph.png'")

    # å¯¼å‡º ONNX æ¨¡å‹
    print("\n============== å¯¼å‡º ONNX æ¨¡å‹ ==============")
    
    # æ ¹æ®è¾“å…¥ç±»å‹é…ç½®è¾“å…¥åç§°å’ŒåŠ¨æ€è½´
    if isinstance(inputs, (tuple, list)):
        input_names = [f"input_{i}" for i in range(len(inputs))]
        dynamic_axes = {f"input_{i}": {0: "batch_size"} for i in range(len(inputs))}
    else:
        input_names = ["input"]
        dynamic_axes = {"input": {0: "batch_size"}}
    
    # é…ç½®è¾“å‡ºåç§°å’ŒåŠ¨æ€è½´
    if isinstance(outputs, (tuple, list)):
        output_names = [f"output_{i}" for i in range(len(outputs))]
        for i in range(len(outputs)):
            dynamic_axes[f"output_{i}"] = {0: "batch_size"}
    else:
        output_names = ["output"]
        dynamic_axes["output"] = {0: "batch_size"}
    
    torch.onnx.export(
        model,
        _input_data,
        onnx_file_path,
        input_names=input_names,
        output_names=output_names,
        dynamic_axes=dynamic_axes,
        opset_version=11,
    )
    print(f"âœ” ONNX æ¨¡å‹å·²ä¿å­˜è‡³ {onnx_file_path}")
    print("åœ¨ https://netron.app/ ä¸ŠæŸ¥çœ‹ ONNX æ¨¡å‹ç»“æ„")

    # # ä½¿ç”¨ ONNX Runtime æ¨ç†
    # print("\n============== ONNX Runtime æ¨ç† ==============")
    # ort_session = onnxruntime.InferenceSession(onnx_file_path)
    # ort_inputs = {
    #     onnx_model.graph.input[i].name: (
    #         inputs[i].cpu().numpy() if isinstance(inputs, (tuple, list))
    #         else inputs.cpu().numpy()
    #     )
    #     for i in range(len(onnx_model.graph.input))
    # }
    # ort_outs = ort_session.run(None, ort_inputs)
    # print(f"ONNX æ¨ç†è¾“å‡ºï¼š{ort_outs}")

if __name__ == "__main__":
    import os
    import numpy as np
    from utils.dataset_prepare import CrashDataset
    from utils.models import TeacherModel, StudentModel
    from utils.weighted_loss import weighted_loss
    
    train_dataset = torch.load(os.path.join("data", "train_dataset.pt"))

    # å®šä¹‰æ¨¡å‹ç›¸å…³çš„è¶…å‚æ•°
    
    Ksize_init = 8 # TCN åˆå§‹å·ç§¯æ ¸å¤§å°ï¼Œå¿…é¡»æ˜¯å¶æ•° 4-12
    Ksize_mid = 5  # TCN ä¸­é—´å·ç§¯æ ¸å¤§å°ï¼Œå¿…é¡»æ˜¯å¥‡æ•° 3 or 5
    num_blocks_of_tcn = 3  # TCN çš„å—æ•° 2 - 6
    tcn_channels_list = [64, 128, 256]  # æ¯ä¸ª TCN å—çš„è¾“å‡ºé€šé“æ•°åˆ—è¡¨
    num_layers_of_mlpE = 3  # MLP ç¼–ç å™¨çš„å±‚æ•° 4-5
    num_layers_of_mlpD = 3  # MLP è§£ç å™¨çš„å±‚æ•° 4-5
    mlpE_hidden = 224  # MLP ç¼–ç å™¨çš„éšè—å±‚ç»´åº¦ 96 - 192
    mlpD_hidden = 160  # MLP è§£ç å™¨çš„éšè—å±‚ç»´åº¦ 128 or 256
    encoder_output_dim = 96  # ç¼–ç å™¨è¾“å‡ºç‰¹å¾ç»´åº¦ 64 or 96
    decoder_output_dim = 16  # è§£ç å™¨è¾“å‡ºç‰¹å¾ç»´åº¦ 16 or 32 or 64
    dropout_TCN = 0.15  # TCN Dropout æ¦‚ç‡ 0.05-0.15
    dropout_MLP = 0.20  # Dropout æ¦‚ç‡ 0.05-0.25
    use_channel_attention=True  # æ˜¯å¦ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶
    fixed_channel_weight = [0.6, 0.4, 0]  # å›ºå®šçš„é€šé“æ³¨æ„åŠ›æƒé‡(Noneè¡¨ç¤ºè‡ªé€‚åº”å­¦ä¹ )

    # å°†æ¨¡å‹ç§»åŠ¨åˆ°CUDAè®¾å¤‡
    # åŠ è½½æ¨¡å‹
    model = TeacherModel(
        Ksize_init=Ksize_init,
        Ksize_mid=Ksize_mid,
        num_classes_of_discrete=train_dataset.dataset.num_classes_of_discrete, # --- ä¿®æ”¹ï¼šä»åŠ è½½çš„è®­ç»ƒé›†ä¸­è·å–å…ƒæ•°æ® ---
        num_blocks_of_tcn=num_blocks_of_tcn,
        tcn_channels_list=tcn_channels_list,
        num_layers_of_mlpE=num_layers_of_mlpE,
        num_layers_of_mlpD=num_layers_of_mlpD,
        mlpE_hidden=mlpE_hidden,
        mlpD_hidden=mlpD_hidden,
        encoder_output_dim=encoder_output_dim,
        decoder_output_dim=decoder_output_dim,
        dropout_MLP=dropout_MLP,
        dropout_TCN=dropout_TCN,
        use_channel_attention=use_channel_attention,
        fixed_channel_weight=fixed_channel_weight
    )

    num_layers_of_mlpE = 3  # MLP ç¼–ç å™¨çš„å±‚æ•°
    num_layers_of_mlpD = 3  # MLP è§£ç å™¨çš„å±‚æ•°
    mlpE_hidden = 224  # MLP ç¼–ç å™¨çš„éšè—å±‚ç»´åº¦
    mlpD_hidden = 160  # MLP è§£ç å™¨çš„éšè—å±‚ç»´åº¦
    encoder_output_dim = 96  # ç¼–ç å™¨è¾“å‡ºç‰¹å¾ç»´åº¦
    decoder_output_dim = 16  # è§£ç å™¨è¾“å‡ºç‰¹å¾ç»´åº¦
    dropout = 0.15  # Dropout æ¦‚ç‡


    # model = StudentModel(
    #     num_classes_of_discrete=dataset.num_classes_of_discrete,
    #     num_layers_of_mlpE=num_layers_of_mlpE, num_layers_of_mlpD=num_layers_of_mlpD,
    #     mlpE_hidden=mlpE_hidden, mlpD_hidden=mlpD_hidden,
    #     encoder_output_dim=encoder_output_dim, decoder_output_dim=decoder_output_dim,
    #     dropout=dropout
    # )

    # modelç§»åŠ¨åˆ°CUDA
    model = model.cuda()

    # ç¤ºä¾‹è¾“å…¥æ•°æ®ï¼ˆæ¨¡æ‹Ÿæ•°æ®é›†ç¬¬1ä¸ªbatchï¼‰
    batch_size = 128

    x_acc = torch.tensor(train_dataset.dataset.x_acc[:batch_size], dtype=torch.float32).cuda()  # (B, 3, 150)
    x_att_con = torch.tensor(train_dataset.dataset.x_att_continuous[:batch_size], dtype=torch.float32).cuda()  # (B, 14)
    x_att_dis = torch.tensor(train_dataset.dataset.x_att_discrete[:batch_size], dtype=torch.long).cuda()  # (B, 4)
    y_HIC = torch.tensor(train_dataset.dataset.y_HIC[:batch_size], dtype=torch.float32).cuda() # (B,)
    y_Dmax = torch.tensor(train_dataset.dataset.y_Dmax[:batch_size], dtype=torch.float32).cuda() # (B,)
    y_Nij = torch.tensor(train_dataset.dataset.y_Nij[:batch_size], dtype=torch.float32).cuda() # (B,)
    y = torch.stack([y_HIC, y_Dmax, y_Nij], dim=1)  # (B, 3)

    criterion = weighted_loss()
    # æµ‹è¯•æ¨¡å‹
    test_model(model, inputs=(x_acc, x_att_con, x_att_dis), labels=y)
    #test_model(model, inputs=(x_att_con, x_att_dis), labels=y)
